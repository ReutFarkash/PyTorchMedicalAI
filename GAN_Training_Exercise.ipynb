{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Training_Exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "FhJVdk_Zwy46",
        "t2LXSMGmw9VA",
        "I9yR2kFCzkFI",
        "doFXB7o4Le5k",
        "JS2R-O-5MLS3"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReutFarkash/PyTorchMedicalAI/blob/master/GAN_Training_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhJVdk_Zwy46",
        "colab_type": "text"
      },
      "source": [
        "# **Generative Adversarial Networks in Pytorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2LXSMGmw9VA",
        "colab_type": "text"
      },
      "source": [
        "## Installing Pytorch and prerequisites "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGF-oOJdopYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q torch==1.0.0 torchvision\n",
        "!pip install -q tqdm\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUpi3yjYpMY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reset -f \n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from shutil import copyfile\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from PIL import Image\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as func\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import random \n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "print('__Python VERSION:', sys.version)\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print('__CUDA VERSION')\n",
        "from subprocess import call\n",
        "# call([\"nvcc\", \"--version\"]) does not work\n",
        "! nvcc --version\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "print('__Devices')\n",
        "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
        "print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "\n",
        "print ('Available devices ', torch.cuda.device_count())\n",
        "print ('Current cuda device ', torch.cuda.current_device())\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# use_cuda = False\n",
        "\n",
        "print(\"USE CUDA=\" + str (use_cuda))\n",
        "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
        "Tensor = FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv4F8NTuxS4Z",
        "colab_type": "text"
      },
      "source": [
        "## Initial Imports and setting CUDA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWnjGr-atNRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm_notebook\n",
        "from torchvision import utils\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "import torch.optim as optim\n",
        "import time as t\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3J8A2AXxY9i",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparametes and Directory creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqdtOK1Jp2t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_size = 64\n",
        "hidden_size = 256\n",
        "image_size = 784\n",
        "num_epochs = 20\n",
        "batch_size = 100\n",
        "sample_dir = 'samples'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CweictQttmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a directory if not exists\n",
        "if not os.path.exists(sample_dir):\n",
        "    os.makedirs(sample_dir)\n",
        "else:\n",
        "    shutil.rmtree(sample_dir)\n",
        "    os.makedirs(sample_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeC6df3lxjgV",
        "colab_type": "text"
      },
      "source": [
        "## Creating a DataLoader for MNIST\n",
        "\n",
        "here you can see how we normalize the mnist data to [-1,1] using minmax normalization, then feeding that transform into the dataset and creating a loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCuE2h1t2gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Lambda(lambda x: (x - 0.5) * 2) # normalize the data to [-1,1]\n",
        "                                ])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ7S-asst4BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = torchvision.datasets.MNIST(root='../../data/',\n",
        "                                   train=True,\n",
        "                                   transform=transform,\n",
        "                                   download=True)\n",
        "\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7N37bRLx5hs",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator\n",
        "A simple Discriminator that is 2 dense layers with leaky relu activations and a dense layer output with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsD62bjHp5ys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = nn.Sequential(\n",
        "    nn.Linear(image_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Linear(hidden_size, 1),\n",
        "    nn.Sigmoid())\n",
        "D = D.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5,0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLavBeGpx96q",
        "colab_type": "text"
      },
      "source": [
        "## Generator\n",
        "A simple Generator that is 2 dense layers with relu activations and a final dense layer with TanH activation (which is constrained to [-1,1])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gie-Fj9x4jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "G = nn.Sequential(\n",
        "    nn.Linear(latent_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, image_size),\n",
        "    nn.Tanh())\n",
        "G = G.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5,0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76bFPRWtyIcX",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop\n",
        "Here we define the training loop of the GAN, a training loop needs to train both the generator and the discriminator at the same time for each batch.\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*hlFyF-klXQunFpmoeA89jQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aye1ndqvp-KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#helper functions\n",
        "def denorm(x):\n",
        "    out = (x + 1) / 2\n",
        "    return out.clamp(0, 1)\n",
        "\n",
        "def reset_grad():\n",
        "    d_optimizer.zero_grad()\n",
        "    g_optimizer.zero_grad()\n",
        "\n",
        "total_step = len(data_loader)\n",
        "for epoch in tqdm_notebook(range(num_epochs)):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        images = images.reshape(batch_size, -1).to(device)\n",
        "        \n",
        "        # Create the labels which are later used as input for the BCE loss\n",
        "        real_labels = torch.ones(batch_size, 1).to(device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
        "\n",
        "        # ================================================================== #\n",
        "        #                      Train the discriminator                       #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
        "        # Second term of the loss is always zero since real_labels == 1\n",
        "        if images.shape[1] != image_size:\n",
        "          continue\n",
        "        \n",
        "        outputs = D(images)\n",
        "        d_loss_real = criterion(outputs, real_labels)\n",
        "        real_score = outputs\n",
        "        \n",
        "        # Compute BCELoss using fake images\n",
        "        # First term of the loss is always zero since fake_labels == 0\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)\n",
        "        fake_score = outputs\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        reset_grad()\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        # ================================================================== #\n",
        "        #                        Train the generator                         #\n",
        "        # ================================================================== #\n",
        "\n",
        "        # Compute loss with fake images\n",
        "        z = torch.randn(batch_size, latent_size).to(device)\n",
        "        fake_images = G(z)\n",
        "        outputs = D(fake_images)\n",
        "        \n",
        "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
        "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "        \n",
        "        # Backprop and optimize\n",
        "        reset_grad()\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "        \n",
        "        if (i+1) == total_step:\n",
        "            print('Epoch [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
        "                  .format(epoch, num_epochs, d_loss.item(), g_loss.item(), \n",
        "                          real_score.mean().item(), fake_score.mean().item()))\n",
        "    \n",
        "    # Save real images\n",
        "    if (epoch+1) == 1:\n",
        "        images = images.reshape(-1, 1, 28, 28)\n",
        "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
        "    # Save sampled images\n",
        "    fake_images = fake_images.reshape(-1, 1, 28, 28)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
        "    fig, ax = plt.subplots(figsize=(20, 10))\n",
        "    img=mpimg.imread(os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
        "    ax.imshow(img)\n",
        "    plt.show()\n",
        "torch.save(G.state_dict(), 'G.ckpt')\n",
        "torch.save(D.state_dict(), 'D.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9yR2kFCzkFI",
        "colab_type": "text"
      },
      "source": [
        "# Training WGAN-GP in Keras with AutoGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mInOCs28Cxu-",
        "colab_type": "text"
      },
      "source": [
        "## Initial Imports and installs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKZdBobyC0fr",
        "colab_type": "code",
        "outputId": "90a89cab-4828-4c45-f0e8-e7569da6bb77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install --no-deps AutoGAN\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting AutoGAN\n",
            "  Downloading https://files.pythonhosted.org/packages/4d/53/406d530d11ed7a241306de98dd2734a8ba59d12d55cd1d266df1d07195b7/AutoGAN-0.0.5-py2.py3-none-any.whl\n",
            "Installing collected packages: AutoGAN\n",
            "Successfully installed AutoGAN-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCT9SlUXDLG3",
        "colab_type": "code",
        "outputId": "462a3406-d332-413b-c407-86c4a927c004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "from AutoGAN import GAN\n",
        "from AutoGAN.schemes.IWGAN_TrainingScheme import IWGAN_TrainingScheme, wasserstein_loss\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Activation\n",
        "from keras.layers import BatchNormalization, ZeroPadding2D, UpSampling2D, Conv2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1sEZEPIz-Qf",
        "colab_type": "text"
      },
      "source": [
        "## Critic\n",
        "Here we define a small deep convolutional network for DCGAN with wasserstein loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Dmmq4Hznvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=(28,28,1), padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU(alpha=0.1))\n",
        "    model.add(Dropout(0.25))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    img = Input(shape=(28, 28, 1))\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img, validity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI1cGyu72iWM",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVWp-7ox2mDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=100))\n",
        "    model.add(Reshape((7, 7, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    \n",
        "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(UpSampling2D())\n",
        "    \n",
        "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    m_noise = Input(shape=(100,))\n",
        "    img = model(m_noise)\n",
        "\n",
        "    return Model(m_noise, img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTjrQC-ADt7a",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbg7UuYeDxw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class save_images(keras.callbacks.Callback):\n",
        "    def __init__(self, model, name='gan'):\n",
        "        super(save_images, self).__init__()\n",
        "        self.full_model = model\n",
        "        self.name = name\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        r, c = 5, 10\n",
        "        local_noise = np.random.normal(0, 1, (r * c, 100))\n",
        "        gen_imgs = self.full_model.generator_model().predict(local_noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "                axs[i, j].axis('off')\n",
        "                cnt += 1\n",
        "        #print(\"saving image: %d\" % epoch)\n",
        "        fig.savefig(\"images/%s/%d.png\" % (self.name,epoch))\n",
        "        plt.show()\n",
        "\n",
        "def load_data():\n",
        "    (real_targets, _), (_, _) = mnist.load_data()\n",
        "    # Rescale -1 to 1\n",
        "    real_targets = real_targets / 127.5 - 1.\n",
        "    real_targets = np.expand_dims(real_targets, axis=3)\n",
        "    noise = np.random.normal(0, 1, (real_targets.shape[0], 100))\n",
        "    return noise, real_targets\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlsqt0rAD1hA",
        "colab_type": "text"
      },
      "source": [
        "## IWGAN Fucntion and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrCx7EkpD3cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iwgan():\n",
        "    model = GAN(generator=build_generator(), discriminator=build_discriminator())\n",
        "    optimizer = Adam(0.00005, 0.5)\n",
        "    optimizerD = Adam(0.0002, 0.5)\n",
        "    try:\n",
        "        shutil.rmtree('images/iwgan')\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        os.makedirs('images/iwgan')\n",
        "    except:\n",
        "        pass\n",
        "    discriminator_kwargs = {'optimizer': optimizerD}\n",
        "    generator_kwargs = {'loss': wasserstein_loss, 'optimizer': optimizer}\n",
        "    model.compile(training_scheme=IWGAN_TrainingScheme(batch_size=128),\n",
        "                  generator_kwargs=generator_kwargs, discriminator_kwargs=discriminator_kwargs)\n",
        "    return model\n",
        "\n",
        "x,y = load_data()\n",
        "model = iwgan()\n",
        "model.fit(x=x, y=y, epochs=20, steps_per_epoch=100, batch_size=128, discriminator_training_multiplier=3,\n",
        "          generator_callbacks=[save_images(model=model, name='iwgan')], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doFXB7o4Le5k",
        "colab_type": "text"
      },
      "source": [
        "# Training Cycle-GAN in Keras with AutoGAN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVKuJPiQM7rb",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhE4kBU0M6sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget -N https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip -O ./apple2orange.zip\n",
        "#!mkdir ./apple2orange/\n",
        "#!unzip -q ./apple2orange.zip -d ./\n",
        "#!rm ./apple2orange.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLZVbtMNB17",
        "colab_type": "text"
      },
      "source": [
        "## Initial imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k175T97OLjxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from AutoGAN import GAN\n",
        "from AutoGAN.schemes.CycleGAN_TrainingScheme import CycleGAN_TrainingScheme\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import UpSampling2D, LeakyReLU, Lambda, Add, Multiply, Activation, Conv2DTranspose\n",
        "from keras.layers import Cropping2D, ZeroPadding2D, Flatten, Subtract, Input, add, multiply\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "from skimage.transform import resize\n",
        "import glob\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew0ECzv6NLhy",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWREU-SSLvJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_discriminator(df, size):\n",
        "\n",
        "    def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if normalization:\n",
        "            d = BatchNormalization()(d)\n",
        "        return d\n",
        "\n",
        "    img = Input(shape=size)\n",
        "\n",
        "    d1 = d_layer(img, df, normalization=False)\n",
        "    d2 = d_layer(d1, df*2)\n",
        "    d3 = d_layer(d2, df*4)\n",
        "    d4 = d_layer(d3, df*8)\n",
        "    d5 = d_layer(d4, df*16)\n",
        "\n",
        "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d5)\n",
        "\n",
        "    return Model(img, validity)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJQqUQoCNPNc",
        "colab_type": "text"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDNlPOHxLvjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(gf, size):\n",
        "    \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "    def conv2d(layer_input, filters, f_size=4):\n",
        "        \"\"\"Layers used during downsampling\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        d = BatchNormalization()(d)\n",
        "        return d\n",
        "\n",
        "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "        \"\"\"Layers used during upsampling\"\"\"\n",
        "        u = UpSampling2D(size=2)(layer_input)\n",
        "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "        if dropout_rate:\n",
        "            u = Dropout(dropout_rate)(u)\n",
        "        u = BatchNormalization()(u)\n",
        "        u = Concatenate()([u, skip_input])\n",
        "        return u\n",
        "\n",
        "    # Image input\n",
        "    d0 = Input(shape=size)\n",
        "\n",
        "    # Downsampling\n",
        "    d1 = conv2d(d0, gf)\n",
        "    d2 = conv2d(d1, gf*2)\n",
        "    d3 = conv2d(d2, gf*4)\n",
        "    d4 = conv2d(d3, gf*8)\n",
        "    d5 = conv2d(d4, gf*16)\n",
        "\n",
        "    # Upsampling\n",
        "    u0 = deconv2d(d5, d4, gf*16)\n",
        "    u1 = deconv2d(u0, d3, gf*4)\n",
        "    u2 = deconv2d(u1, d2, gf*2)\n",
        "    u3 = deconv2d(u2, d1, gf)\n",
        "\n",
        "    u4 = UpSampling2D(size=2)(u3)\n",
        "    output_img = Conv2D(3, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "    return Model(d0, output_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGsQfV_-NOlS",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzXkfMQHL1qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset):\n",
        "    filelist_A = glob.glob('./%s/trainA/*.jpg' % dataset)\n",
        "    shuffle(filelist_A)\n",
        "    A = np.array([plt.imread(file) for file in filelist_A])\n",
        "    print(A.shape)\n",
        "    A = (2. * A/255.) - 1.\n",
        "    \n",
        "    filelist_B = glob.glob('./%s/trainB/*.jpg' % dataset)\n",
        "    shuffle(filelist_B)\n",
        "    B = np.array([plt.imread(file) for file in filelist_B if plt.imread(file).shape[-1] == 3 ])\n",
        "    print(B.shape)\n",
        "    B = (2. * B/255.) - 1.\n",
        "    return A, B\n",
        "\n",
        "def load_data_test(dataset):\n",
        "    filelist_A = glob.glob('./%s/testA/*.jpg' % dataset)\n",
        "    shuffle(filelist_A)\n",
        "    A = np.array([plt.imread(file) for file in filelist_A])\n",
        "    print(A.shape)\n",
        "    A = (2. * A/255.) - 1.\n",
        "    \n",
        "    filelist_B = glob.glob('./%s/testB/*.jpg' % dataset)\n",
        "    shuffle(filelist_B)\n",
        "    B = np.array([plt.imread(file) for file in filelist_B])\n",
        "    print(B.shape)\n",
        "    B = (2. * B/255.) - 1.\n",
        "    return A, B\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXsHI5GcNViF",
        "colab_type": "text"
      },
      "source": [
        "## Image Saver Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yqy18m3L4yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class save_images(keras.callbacks.Callback):\n",
        "    def __init__(self, model, A, B, freq, dataset):\n",
        "        super(save_images, self).__init__()\n",
        "        try:\n",
        "            import os\n",
        "            os.makedirs('images/%s' % dataset)\n",
        "        except:\n",
        "            pass\n",
        "        self.full_model = model\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.epoch = 0\n",
        "        self.freq = freq\n",
        "        self.dataset = dataset\n",
        "    def sample_images(self, epoch, A=None, B=None, end=False):\n",
        "        r, c = 2, 3\n",
        "\n",
        "        # Demo (for GIF)\n",
        "        if A is None:\n",
        "            if 'apple2orange' in self.dataset:\n",
        "                A = np.array([plt.imread('./apple2orange/testA/n07740461_1541.jpg','jpg')])\n",
        "            else:\n",
        "                A = np.array([plt.imread('./horse2zebra/testA/n02381460_1300.jpg','jpg')])\n",
        "            A = (2. * A/255.) - 1.\n",
        "        if B is None:\n",
        "            if 'apple2orange' in self.dataset:\n",
        "                B = np.array([plt.imread('./apple2orange/testB/n07749192_4241.jpg','jpg')])\n",
        "            else:\n",
        "                B = np.array([plt.imread('./horse2zebra/testB/n02391049_9960.jpg','jpg')])\n",
        "            B = (2. * B/255.) - 1.\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_A = self.full_model.generator_model()[0].predict(B)\n",
        "        fake_B = self.full_model.generator_model()[1].predict(A)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.full_model.generator_model()[0].predict(fake_B)\n",
        "        reconstr_B = self.full_model.generator_model()[1].predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([A, fake_B, reconstr_A, B, fake_A, reconstr_B])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        #gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c,figsize=(10,10))\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow((gen_imgs[cnt]+1.)/2.)\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        if end:\n",
        "          try:\n",
        "            shutil.rmtree('./images/%s/final_outputs'% (self.dataset))\n",
        "          except:\n",
        "              pass\n",
        "          try:\n",
        "            os.mkdir('./images/%s/final_outputs'% (self.dataset))\n",
        "          except:\n",
        "              pass\n",
        "            \n",
        "          fig.savefig(\"images/%s/final_outputs/%d.png\" % (self.dataset, epoch))\n",
        "        else:\n",
        "          fig.savefig(\"images/%s/%d.png\" % (self.dataset, epoch))\n",
        "        plt.show()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch = epoch\n",
        "        #print('started epoch %d' % epoch)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.sample_images(self.epoch)\n",
        "    def on_train_end(self, logs=None):\n",
        "        for i in range(1, self.A.shape[0]):\n",
        "            try:\n",
        "                self.sample_images(self.epoch+1, self.A[i-1:i], self.B[i-1:i], True)\n",
        "            except:\n",
        "                continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZEFOecVNdPy",
        "colab_type": "text"
      },
      "source": [
        "## CycleGAN Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huonfZzBL7U-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A, B = load_data('apple2orange')\n",
        "A_test, B_test = load_data_test('apple2orange')\n",
        "\n",
        "def cyclegan(image_A, image_B):\n",
        "    model = GAN(generator=[build_generator(32, image_A.shape),build_generator(32, image_B.shape)], \n",
        "                discriminator=[build_discriminator(32, image_A.shape),build_discriminator(32, image_B.shape)])\n",
        "    optimizer = keras.optimizers.Adam(0.0002, 0.5)\n",
        "    optimizerD = keras.optimizers.Adam(0.0001, 0.5)\n",
        "    try:\n",
        "        shutil.rmtree('./images/apple2orange_cyclegan')\n",
        "    except:\n",
        "        pass\n",
        "    discriminator_kwargs = {'loss':'mse', 'optimizer': optimizerD}\n",
        "    generator_kwargs = {'optimizer': optimizer,\n",
        "                        'translation_weight':1, 'cycle_weight':10, 'identity_weight':1,\n",
        "                        'translation_loss':'mse', 'cycle_loss':'mae', 'identity_loss':'mae'}\n",
        "    model.compile(training_scheme=CycleGAN_TrainingScheme(),\n",
        "                  generator_kwargs=generator_kwargs, discriminator_kwargs=discriminator_kwargs)\n",
        "    return model\n",
        "\n",
        "model = cyclegan(A[0], B[0])\n",
        "#model.summary(True)\n",
        "%matplotlib inline\n",
        "model.fit(x=A, y=B, epochs=20, steps_per_epoch=250, batch_size=1,\n",
        "          generator_callbacks=[save_images(model, A_test, B_test, 100,'apple2orange_cyclegan')], verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS2R-O-5MLS3",
        "colab_type": "text"
      },
      "source": [
        "# Building CycleGAN From Scrach "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJK93YGdM2NT",
        "colab_type": "text"
      },
      "source": [
        "## Downloading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47ShBswM1t1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -N https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/apple2orange.zip -O ./apple2orange.zip\n",
        "!mkdir ./apple2orange/\n",
        "!unzip -q ./apple2orange.zip -d ./\n",
        "!rm ./apple2orange.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQLBjkEM9Bw",
        "colab_type": "text"
      },
      "source": [
        "## Initial Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJEgzeW9M0pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "from AutoGAN import GAN\n",
        "from AutoGAN.schemes.CycleGAN_TrainingScheme import CycleGAN_TrainingScheme\n",
        "\n",
        "import keras\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import UpSampling2D, LeakyReLU, Lambda, Add, Multiply, Activation, Conv2DTranspose\n",
        "from keras.layers import Cropping2D, ZeroPadding2D, Flatten, Subtract, Input, add, multiply\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "from skimage.transform import resize\n",
        "import glob\n",
        "from random import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7kwAvkxM_bO",
        "colab_type": "text"
      },
      "source": [
        "## Creating Networks and Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FgZlnsN7g9d",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEodTOOZ7i0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(dataset):\n",
        "    filelist_A = glob.glob('./%s/trainA/*.jpg' % dataset)\n",
        "    shuffle(filelist_A)\n",
        "    A = np.array([plt.imread(file) for file in filelist_A])\n",
        "    print(A.shape)\n",
        "    A = (2. * A/255.) - 1.\n",
        "    \n",
        "    filelist_B = glob.glob('./%s/trainB/*.jpg' % dataset)\n",
        "    shuffle(filelist_B)\n",
        "    B = np.array([plt.imread(file) for file in filelist_B if plt.imread(file).shape[-1] == 3 ])\n",
        "    print(B.shape)\n",
        "    B = (2. * B/255.) - 1.\n",
        "    return A, B\n",
        "\n",
        "def load_data_test(dataset):\n",
        "    filelist_A = glob.glob('./%s/testA/*.jpg' % dataset)\n",
        "    shuffle(filelist_A)\n",
        "    A = np.array([plt.imread(file) for file in filelist_A])\n",
        "    print(A.shape)\n",
        "    A = (2. * A/255.) - 1.\n",
        "    \n",
        "    filelist_B = glob.glob('./%s/testB/*.jpg' % dataset)\n",
        "    shuffle(filelist_B)\n",
        "    B = np.array([plt.imread(file) for file in filelist_B])\n",
        "    print(B.shape)\n",
        "    B = (2. * B/255.) - 1.\n",
        "    return A, B\n",
        "  \n",
        "A, B = load_data('apple2orange')\n",
        "A_test, B_test = load_data_test('apple2orange')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9WIWyYIUyB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator(gf, size):\n",
        "    \"\"\"U-Net Generator\"\"\"\n",
        "\n",
        "    def conv2d(layer_input, filters, f_size=4):\n",
        "        \"\"\"Layers used during downsampling\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        d = BatchNormalization()(d)\n",
        "        return d\n",
        "\n",
        "    def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
        "        \"\"\"Layers used during upsampling\"\"\"\n",
        "        u = UpSampling2D(size=2)(layer_input)\n",
        "        u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
        "        if dropout_rate:\n",
        "            u = Dropout(dropout_rate)(u)\n",
        "        u = BatchNormalization()(u)\n",
        "        u = Concatenate()([u, skip_input])\n",
        "        return u\n",
        "\n",
        "    # Image input\n",
        "    d0 = Input(shape=size)\n",
        "\n",
        "    # Downsampling\n",
        "    d1 = conv2d(d0, gf)\n",
        "    d2 = conv2d(d1, gf*2)\n",
        "    d3 = conv2d(d2, gf*4)\n",
        "    d4 = conv2d(d3, gf*8)\n",
        "    d5 = conv2d(d4, gf*16)\n",
        "\n",
        "    # Upsampling\n",
        "    u0 = deconv2d(d5, d4, gf*16)\n",
        "    u1 = deconv2d(u0, d3, gf*4)\n",
        "    u2 = deconv2d(u1, d2, gf*2)\n",
        "    u3 = deconv2d(u2, d1, gf)\n",
        "\n",
        "    u4 = UpSampling2D(size=2)(u3)\n",
        "    output_img = Conv2D(3, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
        "\n",
        "    return Model(d0, output_img)\n",
        "\n",
        "def build_discriminator(df, size):\n",
        "\n",
        "    def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
        "        \"\"\"Discriminator layer\"\"\"\n",
        "        d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
        "        d = LeakyReLU(alpha=0.2)(d)\n",
        "        if normalization:\n",
        "            d = BatchNormalization()(d)\n",
        "        return d\n",
        "\n",
        "    img = Input(shape=size)\n",
        "\n",
        "    d1 = d_layer(img, df, normalization=False)\n",
        "    d2 = d_layer(d1, df*2)\n",
        "    d3 = d_layer(d2, df*4)\n",
        "    d4 = d_layer(d3, df*8)\n",
        "    d5 = d_layer(d4, df*16)\n",
        "\n",
        "    validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d5)\n",
        "\n",
        "    return Model(img, validity) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YHxUVygNDnE",
        "colab_type": "text"
      },
      "source": [
        "## Building Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHlbf0bjmEPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_ab, generator_ba = build_generator(32, A[0].shape),build_generator(32, B[0].shape)\n",
        "discriminator_a, discriminator_b = build_discriminator(32, A[0].shape),build_discriminator(32, B[0].shape)\n",
        "optimizerG = keras.optimizers.Adam(0.0002, 0.5)\n",
        "optimizer = keras.optimizers.Adam(0.0001, 0.5)\n",
        "\n",
        "epochs = 20\n",
        "steps = 100\n",
        "\n",
        "## YOUR CODE HERE!\n",
        "loss_fake =\n",
        "loss_cycle =\n",
        "loss_id =\n",
        "\n",
        "w_fake = 1\n",
        "w_cycle = 10\n",
        "w_id = 1\n",
        "## END HERE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi1zQ310nQ3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "input_A = keras.layers.Input(A[0].shape)\n",
        "input_B = keras.layers.Input(B[0].shape)\n",
        "\n",
        "fake_B = generator_ab(input_A)\n",
        "fake_A = generator_ba(input_B)\n",
        "\n",
        "## YOUR CODE HERE!\n",
        "dis_fake_A = \n",
        "dis_fake_B = \n",
        "\n",
        "dis_real_A = \n",
        "dis_real_B = \n",
        "\n",
        "reconstructed_A = \n",
        "reconstructed_B = \n",
        "\n",
        "Identity_A = \n",
        "Identity_B = \n",
        "## END HERE\n",
        "\n",
        "\n",
        "\n",
        "model_gen = keras.models.Model(inputs=[input_A, input_B], outputs=[dis_fake_A, dis_fake_B, \n",
        "                                                                   reconstructed_A, reconstructed_B,\n",
        "                                                                   Identity_A, Identity_B])\n",
        "model_gen.compile(optimizer=optimizerG, loss=[loss_fake, loss_fake,\n",
        "                                              loss_cycle, loss_cycle,\n",
        "                                              loss_id, loss_id],\n",
        "                  loss_weights=[w_fake, w_fake, w_cycle, w_cycle, w_id, w_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjliCUAfNHXR",
        "colab_type": "text"
      },
      "source": [
        "## Building Discriminators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqs5liIcoxxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dis = keras.models.Model(inputs=[input_A, input_B], outputs=[dis_real_A, dis_real_B, dis_fake_A, dis_fake_B])\n",
        "model_dis.compile(optimizer=optimizer, loss=loss_fake)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMELkuqMNKsy",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions and Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze5SgTf2vM82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class save_images_full(keras.callbacks.Callback):\n",
        "    def __init__(self, model, A, B, freq, dataset):\n",
        "        super(save_images_full, self).__init__()\n",
        "        try:\n",
        "            import os\n",
        "            os.makedirs('images/%s' % dataset)\n",
        "        except:\n",
        "            pass\n",
        "        self.full_model = model\n",
        "        self.A = A\n",
        "        self.B = B\n",
        "        self.epoch = 0\n",
        "        self.freq = freq\n",
        "        self.dataset = dataset\n",
        "    def sample_images(self, epoch, A=None, B=None, end=False):\n",
        "        r, c = 2, 3\n",
        "\n",
        "        # Demo (for GIF)\n",
        "        if A is None:\n",
        "            if 'apple2orange' in self.dataset:\n",
        "                A = np.array([plt.imread('./apple2orange/testA/n07740461_1541.jpg','jpg')])\n",
        "            else:\n",
        "                A = np.array([plt.imread('./horse2zebra/testA/n02381460_1300.jpg','jpg')])\n",
        "            A = (2. * A/255.) - 1.\n",
        "        if B is None:\n",
        "            if 'apple2orange' in self.dataset:\n",
        "                B = np.array([plt.imread('./apple2orange/testB/n07749192_4241.jpg','jpg')])\n",
        "            else:\n",
        "                B = np.array([plt.imread('./horse2zebra/testB/n02391049_9960.jpg','jpg')])\n",
        "            B = (2. * B/255.) - 1.\n",
        "\n",
        "        # Translate images to the other domain\n",
        "        fake_A = self.full_model[0].predict(B)\n",
        "        fake_B = self.full_model[1].predict(A)\n",
        "        # Translate back to original domain\n",
        "        reconstr_A = self.full_model[0].predict(fake_B)\n",
        "        reconstr_B = self.full_model[1].predict(fake_A)\n",
        "\n",
        "        gen_imgs = np.concatenate([A, fake_B, reconstr_A, B, fake_A, reconstr_B])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        #gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        titles = ['Original', 'Translated', 'Reconstructed']\n",
        "        fig, axs = plt.subplots(r, c,figsize=(10,10))\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow((gen_imgs[cnt]+1.)/2.)\n",
        "                axs[i, j].set_title(titles[j])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        if end:\n",
        "          try:\n",
        "            shutil.rmtree('./images/%s/final_outputs'% (self.dataset))\n",
        "          except:\n",
        "              pass\n",
        "          try:\n",
        "            os.mkdir('./images/%s/final_outputs'% (self.dataset))\n",
        "          except:\n",
        "              pass\n",
        "            \n",
        "          fig.savefig(\"images/%s/final_outputs/%d.png\" % (self.dataset, epoch))\n",
        "        else:\n",
        "          fig.savefig(\"images/%s/%d.png\" % (self.dataset, epoch))\n",
        "        plt.show()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch = epoch\n",
        "        #print('started epoch %d' % epoch)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.sample_images(self.epoch)\n",
        "    def on_train_end(self, logs=None):\n",
        "        for i in range(1, self.A.shape[0]):\n",
        "            try:\n",
        "                self.sample_images(self.epoch+1, self.A[i-1:i], self.B[i-1:i], True)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "image_saver = save_images_full([generator_ab, generator_ba], A_test, B_test, 100,'apple2orange_full_cyclegan')\n",
        "\n",
        "def get_batch():\n",
        "  while 1:\n",
        "    np.random.shuffle(A)\n",
        "    np.random.shuffle(B)\n",
        "    for a_batch, b_batch in zip(A,B):\n",
        "      yield np.expand_dims(a_batch, 0), np.expand_dims(b_batch, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mji2PzMfNJ8H",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8BuD_pxmX-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_gen = get_batch()\n",
        "for epoch in tqdm_notebook(range(epochs)):\n",
        "  epoch_loss_dis = None\n",
        "  epoch_loss_gen = None\n",
        "  for step in tqdm_notebook(range(steps)):\n",
        "    a, b = next(data_gen)\n",
        "    ones = np.ones([1] + model_dis.outputs[0].get_shape().as_list()[1:])\n",
        "    zeros = np.zeros([1] + model_dis.outputs[0].get_shape().as_list()[1:])\n",
        "    \n",
        "    ## YOUR CODE HERE!\n",
        "    loss_dis = model_dis.train_on_batch(x=[], y=[])\n",
        "    loss_gen = model_gen.train_on_batch(x=[], y=[])\n",
        "    ## END HERE\n",
        "    if epoch_loss_dis is None:\n",
        "      epoch_loss_dis = loss_dis\n",
        "    else:\n",
        "      epoch_loss_dis = epoch_loss_dis + loss_dis if not isinstance(epoch_loss_dis, list)\\\n",
        "      else [l1+l2 for l1, l2 in zip(epoch_loss_dis, loss_dis)]     \n",
        "    if epoch_loss_gen is None:\n",
        "      epoch_loss_gen = loss_gen\n",
        "    else:\n",
        "      epoch_loss_gen = epoch_loss_gen + loss_gen if not isinstance(epoch_loss_gen, list)\\\n",
        "      else [l1+l2 for l1, l2 in zip(epoch_loss_gen, loss_gen)]\n",
        "     \n",
        "  epoch_loss_gen = epoch_loss_gen / steps if not isinstance(epoch_loss_gen, list) else [l/steps for l in epoch_loss_gen]\n",
        "  epoch_loss_dis = epoch_loss_dis / steps if not isinstance(epoch_loss_dis, list) else [l/steps for l in epoch_loss_dis]\n",
        "  print (\"Epoch:\", epoch,\"\\nGenerator Loss: \", epoch_loss_gen, \"\\nDiscriminator Loss: \", epoch_loss_dis)\n",
        "  image_saver.on_epoch_end(epoch)\n",
        "    \n",
        "image_saver.on_train_end()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}